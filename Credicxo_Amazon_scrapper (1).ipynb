{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credicxo Amazon scrapper",
      "provenance": [],
      "collapsed_sections": [
        "lnWeibtSYup0",
        "TgXZaDhRZC63",
        "QADrYEy9bXHF",
        "h5maFfhU_YVQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#`Credicxo` Amazon scrapper [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1MPAmJaJLXxE_iDLdDVdJoGu6NZ8xthZR?usp=sharing)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xENXr6NgYVFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Importing all the required libraries "
      ],
      "metadata": {
        "id": "lnWeibtSYup0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Installing mysql-connector library...')\n",
        "!pip install mysql-connector"
      ],
      "metadata": {
        "id": "5qyyiySTILaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import\n",
        "import concurrent.futures\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import mysql.connector\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HCAzY9AUMtwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Creating the scrapper\n",
        "    \n",
        "\n",
        "*  Generate urls froom the csv by `urls_gen` function\n",
        "*  Scrap the the data and store to `amazon_data` list\n",
        "*  Dumps the `amazon_data` to `Amazon_Products.json`\n",
        "*  And, dumps all the `amazon_data` to a mysql db\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TgXZaDhRZC63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CredicxoScrap:\n",
        "  \"\"\" Data scrap from amazon sites\n",
        "  \"\"\"\n",
        "  def __init__(self, csv_path):\n",
        "    self.csv_path = csv_path\n",
        "\n",
        "  def urls_gen(self):\n",
        "    \"\"\"urls generetor form the csv\"\"\"\n",
        "\n",
        "    csv = pd.read_csv(self.csv_path) #csv_file\n",
        "\n",
        "    asin = csv.Asin.values.tolist()  #extract asin from the csv\n",
        "    country = csv.country.values.tolist()  ##extract country from the csv\n",
        "    \n",
        "    #saving teh urls\n",
        "    all_urls = [f'https://www.amazon.{country}/dp/{asin}'\\\n",
        "                for asin, country in zip(asin,country)]\n",
        "    print('Printing the first url form the list: {}'.format(all_urls[0]))\n",
        "    return all_urls\n",
        "\n",
        "  @staticmethod\n",
        "  def html_gen(url):\n",
        "    \"\"\"generate html(html.parser) form the url\"\"\"\n",
        "\n",
        "    r = requests.get(url,\n",
        "                      headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) \\\n",
        "                      Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\\\n",
        "                      \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\\\n",
        "                      \"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"})\n",
        "    \n",
        "    #checking for 404 error\n",
        "    if r.status_code==404:\n",
        "      print(f'The {url} is unavilable. Skipping...')\n",
        "    else:\n",
        "      html = soup(r.content, 'html.parser') #html.parser\n",
        "      \n",
        "      return html\n",
        "    \n",
        "  \n",
        "  def scrap_data(self, url):\n",
        "    \"\"\"scrap data from the url\"\"\"\n",
        "\n",
        "    html = self.html_gen(url) #create html from the link\n",
        "\n",
        "    product_row = {}\n",
        "    product_row['Product Title'] = html.find('span', id='productTitle').text  #scrap product title\n",
        "    product_row['Product Image URL'] = html.find('div', class_='imgTagWrapper').find('img')['src'] #scrap product image url\n",
        "    product_row['Price of the Product'] = html.find('span', class_='a-price aok-align-center priceToPay').text[1:] #scrap product price\n",
        "    product_row['Product Details'] = html.find('div', id='feature-bullets').text  #scrap product details\n",
        "\n",
        "    return product_row  #append the product row to amazon data list\n",
        "  \n"
      ],
      "metadata": {
        "id": "aWKbKIKsM289"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#csv_path\n",
        "csv_path = '/content/Amazon Scraping - Sheet1.csv'\n",
        "\n",
        "#class with csv_path\n",
        "amazon = CredicxoScrap(csv_path)\n",
        "\n",
        "#save all_urls\n",
        "all_urls = amazon.urls_gen()"
      ],
      "metadata": {
        "id": "BR9ejRv9Mg0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scrap data from url\n",
        "amazon_data = []\n",
        "\n",
        "#scraped data to store to amazon_data with multithreading\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  executor.map(amazon.scrap_data(), all_urls)\n",
        "\n",
        "#for measuring the time for each 100 urls\n",
        "end_time = time.perf_counter()\n",
        "print(\"Each round of 100 urls is taking: \"+str(round((start_time-end_time)/10, 2)))\n",
        "    "
      ],
      "metadata": {
        "id": "VBA1D5L9MqiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Createing Json file\n"
      ],
      "metadata": {
        "id": "QADrYEy9bXHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def json_handler():\n",
        "  \"\"\"create json file from the list of dict\"\"\"\n",
        "  currnet_path = os.getcwd()  #extract current path\n",
        "  json_path = os.path.join(currnet_path, 'Amazon_Products.json')   # creating json path\n",
        "  with open(json_path, 'w') as f:\n",
        "    json.dump(amazon_data, f, indent=4)   # dumps the scraped data\n",
        "  \n",
        "  print(json_path)\n",
        "\n",
        "json_handler()"
      ],
      "metadata": {
        "id": "fdk14SfUM9N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UUixz-fT_Xz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Storing the data to MySQL"
      ],
      "metadata": {
        "id": "h5maFfhU_YVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list object store to database\n",
        "def mysql_handler(self,host=\"localhost\",user=\"root\",\n",
        "                  password=\"root\",database=\"amazonscrap\"):\n",
        "    \n",
        "  \"\"\"stroing data in mysql db from the list of dict\"\"\"\n",
        "\n",
        "  db = mysql.connector.connect(host=host,user=user,\n",
        "                              password=password,database=database) #connect to a databse\n",
        "    \n",
        "  cursor = db.cursor()  #db cursor\n",
        "\n",
        "  cusrsor.excute(\"CREATE TABLE amazon (title varchar(100), img_url varchar(100), price int, details varchar(1000)) \")  #creating columns\n",
        "\n",
        "  sql = \"INSERT INTO amazon (title, img_url, price, details) VALUES (%s, %s, %s, %s)\"  #sql\n",
        "\n",
        "\n",
        "  try:\n",
        "    cursor.executemany(sql, amazon_data)\n",
        "    db.commit()  \n",
        "  except:\n",
        "    cursor.rollback() \n",
        "\n",
        "  print(cursor.rowcount, \"record inserted.\")\n",
        "\n",
        "mysql_handler()"
      ],
      "metadata": {
        "id": "urDkA7jIQFHC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}